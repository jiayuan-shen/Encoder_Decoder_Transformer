{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18EwPJO_vc9EpAsjJ_RJfAI7QAQtME4R7","timestamp":1710804415106},{"file_id":"1DyG896PKCXEbpJsuUVBB8Oyn8oz88TFY","timestamp":1710630743306}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"l1HhWAQWNSdV"},"outputs":[],"source":["import math\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-C0tSgULsrgS","executionInfo":{"status":"ok","timestamp":1710650230738,"user_tz":240,"elapsed":15007,"user":{"displayName":"秦乔","userId":"01985652240098165642"}},"outputId":"0065d559-441e-4e58-8c77-54e68ed5f32f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# def get_vocab(vocab_path):\n","#   with open(vocab_path,'r') as f:\n","#     vocab = json.load(f)\n","#     vocab.append('<SOS>')\n","#     vocab.append('<EOS>')\n","#     stoi = {s:(i+1) for i,s in enumerate(vocab)}\n","#     itos = {(i+1):s for i,s in enumerate(vocab)}\n","#     stoi['<PAD>'] = 0\n","#     itos[0] = '<PAD>'\n","#     return stoi,itos\n","\n","# stoi_en,itos_en = get_vocab(r'/content/drive/MyDrive/2024Spring/641NaturalLanguageProcessing/NewFolder/vocab_en.json')\n","# stoi_zh,itos_zh = get_vocab(r'/content/drive/MyDrive/2024Spring/641NaturalLanguageProcessing/NewFolder/vocab_zh.json')"],"metadata":{"id":"jmZ3IRD4Nz_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # hyperparameters\n","# # model\n","# vocab_size_en = len(stoi_en)\n","# vocab_size_zh = len(stoi_zh)\n","# max_length = 32         # max length of the input sequence\n","# n_emb = 8               # embedding size\n","# n_head = 2              # number of heads in multi-head attention\n","# head_size = 4           # number of 'features' output by a single-head self-attention\n","# n_blocks = 1            # number of blocks in a encoder or decoder\n","# n_hidden = 2048\n","# assert head_size*n_head == n_emb, ''\n","\n","# # training\n","# batch_size = 32\n","# learning_rate = 1e-3\n","# device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"9k1fsIS2ozDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class embedding(nn.Module):\n","  def __init__(self,vocab_size,n_emb,max_len):\n","    super().__init__()\n","    self.n_emb = n_emb\n","\n","    self.word_embedding = nn.Embedding(vocab_size,n_emb)\n","\n","    pe = torch.zeros(max_len, n_emb)\n","    position = torch.unsqueeze(torch.arange(0, max_len, dtype=torch.float),dim=1)\n","    div_term = torch.exp(torch.arange(0, n_emb, 2).float() * (-math.log(10000.0) / n_emb))\n","    pe[:, 0::2] = torch.sin(position * div_term)\n","    pe[:, 1::2] = torch.cos(position * div_term)\n","    pe = pe.unsqueeze(0)  # Add batch dimension\n","    self.register_buffer('pe', pe)\n","\n","  def forward(self,x):\n","    word_emb = self.word_embedding(x) * math.sqrt(self.n_emb)         # [B,T,n_emb]\n","    pos_emb = self.pe[:,:word_emb.size(1),:]                           # [T,n_emb]\n","    return pos_emb + word_emb                                         # [B,T,n_emb]"],"metadata":{"id":"CorHNPMITXCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test embedding\n","# x = torch.randint(low=0,high=20,size=(2,5))\n","# emb = embedding(vocab_size_en,n_emb,max_length)\n","# out = emb(x)\n","# out.shape"],"metadata":{"id":"ppkCmfFDVH0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710651414149,"user_tz":240,"elapsed":150,"user":{"displayName":"秦乔","userId":"01985652240098165642"}},"outputId":"b28386cf-d344-4207-cfa4-e5a185e4584e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 5, 8])\n","torch.Size([1, 32, 8])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 5, 8])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# single-head self-attention\n","class selfAttention(nn.Module):\n","  def __init__(self,fan_in,fan_out,masked):\n","    super().__init__()\n","    self.query = nn.Linear(fan_in,fan_out)\n","    self.key = nn.Linear(fan_in,fan_out)\n","    self.value = nn.Linear(fan_in,fan_out)\n","    self.masked = masked\n","  def forward(self,x):\n","    B,T,C = x.shape\n","    q = self.query(x)   # [B,T,fan_out]\n","    k = self.key(x)     # [B,T,fan_out]\n","    v = self.value(x)   # [B,T,fan_out]\n","    attention_score = q @ torch.transpose(k,dim0=1,dim1=2) * C**(-0.5)  # [B,T,T]\n","    if self.masked:\n","      mask = torch.tril(attention_score)\n","      attention_score = attention_score.masked_fill(mask==0,float('-inf'))\n","      attention_score = F.softmax(attention_score,dim=-1)\n","    out = attention_score @ v # [B,T,fan_out]\n","    return out"],"metadata":{"id":"-fMEImkxNelk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# single-head unmasked cross-attention for decoder\n","class crossAttention(nn.Module):\n","  def __init__(self,fan_in,fan_out):\n","    super().__init__()\n","    self.query = nn.Linear(fan_in,fan_out)\n","    self.key = nn.Linear(fan_in,fan_out)\n","    self.value = nn.Linear(fan_in,fan_out)\n","  def forward(self,x,cross):\n","    B,T,C = x.shape\n","    q = self.query(x)\n","    k = self.key(cross)\n","    v = self.value(cross)\n","    attention_score = q @ torch.transpose(k,dim0=1,dim1=2) * C**(-0.5)\n","    out = attention_score @ v\n","    return out"],"metadata":{"id":"3MlIZPD6g_3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# multi-head self-attention\n","class multiHead_sa(nn.Module):\n","  def __init__(self,fan_in,fan_out,masked,n_head):\n","    super().__init__()\n","    self.multi_head = nn.ModuleList([selfAttention(fan_in,fan_out,masked)]*n_head)\n","\n","  def forward(self,x):\n","    out = None\n","    for i,head in enumerate(self.multi_head):\n","      if i == 0:\n","        out = head(x)\n","      else:\n","        out = torch.concat([out,head(x)],dim=-1)\n","\n","    return out"],"metadata":{"id":"xQfgRifLaSPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# multi-head cross-attention\n","class multiHead_ca(nn.Module):\n","  def __init__(self,fan_in,fan_out,n_head):\n","    super().__init__()\n","    self.multi_head = nn.ModuleList([crossAttention(fan_in,fan_out)]*n_head)\n","\n","  def forward(self,x,cross):\n","    out = None\n","    for i,head in enumerate(self.multi_head):\n","      if i == 0:\n","        out = head(x,cross)\n","      else:\n","        out = torch.concat([out,head(x,cross)],dim=-1)\n","    return out"],"metadata":{"id":"XZ18s5oJZSjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoder block\n","class encoderBlock(nn.Module):\n","  def __init__(self,fan_in,fan_out,n_head):\n","    super().__init__()\n","    self.multi_head = multiHead_sa(fan_in,fan_out,False,n_head)\n","    self.layerNorm = nn.LayerNorm(fan_in)\n","    self.ffw = nn.Linear(fan_in,fan_in)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self,x):\n","    out = x + self.relu(self.multi_head(x))\n","    out = self.layerNorm(out)\n","    out = out + self.relu(self.ffw(out))\n","    out = self.layerNorm(out)\n","    return out"],"metadata":{"id":"wPkwONN5cGoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# decoder block\n","class decoderBlock(nn.Module):\n","  def __init__(self,fan_in,fan_out,n_head):\n","    super().__init__()\n","    self.masked_multihead = multiHead_sa(fan_in,fan_out,True,n_head)\n","    self.layerNorm = nn.LayerNorm(fan_in)\n","    self.cross_multihead = multiHead_ca(fan_in,fan_out,n_head)\n","    self.ffw = nn.Linear(fan_in,fan_in)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self,x,cross):\n","    out = x + self.relu(self.masked_multihead(x))\n","    out = self.layerNorm(out)\n","    out = out + self.relu(self.cross_multihead(out,cross))\n","    out = self.layerNorm(out)\n","    out = out + self.relu(self.ffw(out))\n","    out = self.layerNorm(out)\n","    return out"],"metadata":{"id":"0ErfsYpBGrW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoder\n","class Encoder(nn.Module):\n","  def __init__(self,n_emb,head_size,n_head,n_blocks):\n","    super().__init__()\n","    self.blocks = nn.ModuleList([encoderBlock(n_emb,head_size,n_head)]*n_blocks)\n","\n","  def forward(self,x):\n","    crosses = []\n","    for block in self.blocks:\n","      crosses.append(block(x))\n","    return crosses"],"metadata":{"id":"g8LQaI82KmmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# decoder\n","class Decoder(nn.Module):\n","  def __init__(self,n_emb,head_size,n_head,n_blocks,vocab_size):\n","    super().__init__()\n","    self.blocks = nn.ModuleList([decoderBlock(n_emb,head_size,n_head)] * n_blocks)\n","    self.linear = nn.Linear(n_emb,vocab_size)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self,x,crosses):\n","    for i,cross in enumerate(crosses):\n","      if i == 0:\n","        out = self.blocks[i](x,cross)\n","      else:\n","        out = self.blocks[i](out,cross)\n","    out = self.relu(self.linear(out))\n","    return out"],"metadata":{"id":"LDRfmZ6iKmjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transformer encoder-decoder\n","class Transformer(nn.Module):\n","  def __init__(self,n_emb,head_size,n_head,n_blocks,vocab_size_enc,vocab_size_dec,max_len):\n","    super().__init__()\n","    self.embedding_enc = embedding(vocab_size_enc,n_emb,max_len)\n","    self.embedding_dec = embedding(vocab_size_dec,n_emb,max_len)\n","    self.encoder = Encoder(n_emb,head_size,n_head,n_blocks)\n","    self.decoder = Decoder(n_emb,head_size,n_head,n_blocks,vocab_size_dec)\n","    self.max_len = max_len\n","\n","  def forward(self,seq_enc,seq_dec):\n","    emb_enc = self.embedding_enc(seq_enc)\n","    emb_dec = self.embedding_dec(seq_dec)\n","    crosses = self.encoder(emb_enc)\n","    out = self.decoder(emb_dec,crosses)\n","    return out\n","\n","  def generate(self,seq_enc,seq_dec):\n","    seq_enc = torch.unsqueeze(seq_enc,dim=0)\n","    while itos_zh[seq_dec[-1].item()] != '<EOS>':\n","      seq_dec = torch.unsqueeze(seq_dec,dim=0)\n","      out = self(seq_enc,seq_dec) # [1,T_dec,vocab_size]\n","      out = out[:,-1,:]           # [1,vocab_size]\n","      prob = F.softmax(out,-1)\n","      out = torch.multinomial(prob,num_samples=1,replacement=True)  # [1,1]\n","      seq_dec = torch.concat([seq_dec,out],dim=-1).squeeze()  # [T_dec+1]\n","      if seq_dec.shape[0] >= self.max_len:\n","        break\n","\n","    return seq_dec"],"metadata":{"id":"hCSOVG13oKCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test selfAttention\n","# x = torch.randn((2,5,32))\n","# sa_masked = selfAttention(32,8,True)\n","# sa_unmasked = selfAttention(32,8,False)\n","# out_masked = sa_masked(x)\n","# out_unmasked = sa_unmasked(x)\n","# out_masked.shape,out_unmasked.shape"],"metadata":{"id":"JeNx0hS0Yuiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test multi-head self-attention\n","# x = torch.randn((2,5,32))\n","# mhsa = multiHead_sa(32,8,True,n_head)\n","# out = mhsa(x)\n","# out.shape"],"metadata":{"id":"WTXHTHWOaunC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test multi-head cross-attention\n","# x = torch.randn((2,5,32))\n","# cross = torch.randn((2,5,32))\n","# mhca = multiHead_ca(32,8,4)\n","# out = mhca(x,cross)\n","# out.shape"],"metadata":{"id":"zOp67QnYJfCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test encoder block\n","# x = torch.randn((2,5,32))\n","# en_block = encoderBlock(32,8,4)\n","# out = en_block(x)\n","# out.shape"],"metadata":{"id":"ZVG1hwNnKFOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test decoder block\n","# x = torch.randn((2,5,32))\n","# cross = torch.randn((2,5,32))\n","# de_block = decoderBlock(32,8,4)\n","# out = de_block(x,cross)\n","# out.shape"],"metadata":{"id":"BRfmRubgKWQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test Encoder\n","# x = torch.randn((2,5,32))\n","# encoder = Encoder(32,8,4,3)\n","# crosses = encoder(x)\n","# len(crosses)"],"metadata":{"id":"TZUKBlbdNR_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test Decoder\n","# x = torch.randn((2,5,32))\n","# decoder = Decoder(32,8,4,3,vocab_size_zh)\n","# out = decoder(x,crosses)\n","# out.shape"],"metadata":{"id":"h5GIj8aqNsox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test Transformer\n","# seq_enc = torch.randint(0,100,(2,4))\n","# seq_dec = torch.randint(0,100,(2,7))\n","# transformer = Transformer(n_emb,head_size,n_head,n_blocks,vocab_size_en,vocab_size_zh,max_length)\n","# out = transformer(seq_enc,seq_dec)"],"metadata":{"id":"nUlCOOgtr532"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TorchTransformer(nn.Module):\n","  def __init__(self,n_emb,head_size,n_head,n_blocks,vocab_size_enc,vocab_size_dec,n_hidden,max_len):\n","    super().__init__()\n","    self.embedding_enc = embedding(vocab_size_enc,n_emb,max_len)\n","    self.embedding_dec = embedding(vocab_size_dec,n_emb,max_len)\n","    self.transformer = nn.Transformer(d_model=n_emb,nhead=n_head,num_encoder_layers=n_blocks,num_decoder_layers=n_blocks,dim_feedforward=n_hidden,batch_first=True)\n","    self.linear = nn.Linear(n_emb,vocab_size_dec)\n","    self.max_len = max_len\n","\n","  def forward(self,seq_enc,seq_dec,mask_enc=None,mask_dec=None,mask_enc_padding=None,mask_dec_padding=None,memory_key_padding_mask=None):\n","    emb_enc = self.embedding_enc(seq_enc)\n","    emb_dec = self.embedding_dec(seq_dec)\n","    out = self.transformer(src=emb_enc,tgt=emb_dec,\n","                           src_mask=mask_enc,tgt_mask=mask_dec,\n","                           src_key_padding_mask=mask_enc_padding,tgt_key_padding_mask=mask_dec_padding,\n","                           memory_key_padding_mask=memory_key_padding_mask)\n","    out = self.linear(out)\n","    return out\n","\n","  def encode(self,seq_enc,mask_enc=None):\n","    emb_enc = self.embedding_enc(seq_enc)\n","    return self.transformer.encoder(emb_enc,mask_enc)\n","\n","  def decode(self,seq_dec,memory,mask_dec):\n","    emb_dec = self.embedding_dec(seq_dec)\n","    return self.transformer.decoder(emb_dec,memory,mask_dec)\n","\n","  def generate(self,seq_enc,seq_dec,test=False):\n","    seq_enc = torch.unsqueeze(seq_enc,dim=0)\n","    while itos_zh[seq_dec[-1].item()] != '<EOS>':\n","      seq_dec = torch.unsqueeze(seq_dec,dim=0)\n","      out = self(seq_enc,seq_dec) # [1,T_dec,vocab_size]\n","      out = out[:,-1,:]           # [1,vocab_size]\n","      prob = F.softmax(out,-1)\n","      out = torch.multinomial(prob,num_samples=1)        # [1,1]\n","      temp = seq_dec\n","      seq_dec = torch.concat([seq_dec,out],dim=-1).squeeze()  # [T_dec+1]\n","\n","      if test:\n","        print('Use ')\n","        print('input sequence: ', seq_enc)\n","        print('output sequence: ', temp)\n","        print('to predict', out)\n","        print('***************************')\n","      if seq_dec.shape[0] >= self.max_len:\n","        break\n","\n","    return seq_dec"],"metadata":{"id":"mxfBcdPrEHrC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test TorchTransformer\n","# import spacy\n","# nlp = spacy.load('en_core_web_sm')\n","\n","# def generate(input_seq,test=False):\n","#   tokens = nlp(input_seq)\n","#   tokens = [stoi_en['<SOS>']] + [stoi_en[token.text.lower()] for token in tokens] + [stoi_en['<EOS>']]\n","#   seq_enc = torch.tensor(tokens).to(device)\n","#   seq_dec = torch.tensor([stoi_zh['<SOS>']]).to(device)\n","#   output = model.generate(seq_enc,seq_dec,test)\n","#   output = [itos_zh[o.item()] for o in output][1:]\n","#   return ''.join(output)\n","\n","# model = TorchTransformer(n_emb,head_size,n_head,n_blocks,vocab_size_en,vocab_size_zh,n_hidden,max_length)\n","# input_seq = \"harry potter\"\n","# output_seq = generate(input_seq,test=True)"],"metadata":{"id":"TDQ0hWcEdrP4"},"execution_count":null,"outputs":[]}]}